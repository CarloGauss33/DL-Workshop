{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creando tu Primera Red Neuronal con Pytorch\n",
    "\n",
    "### Overview\n",
    "\n",
    "Tal como vimos en los contenidos la red poseera la siguente estructura\n",
    "\n",
    "**Input -> Conv -> ReLU -> MaxPool -> Conv -> ReLU -> MaxPool -> View -> Linear -> ReLU -> Linear -> ReLU -> Linear -> MSELoss -> Loss**\n",
    "\n",
    "\n",
    "\n",
    "#### Definiendo Nuestra Red\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# importamos pytorch y sus clases de redes neuronales\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Definimos la clase de red heredando desde la clase madre de Neural Networks\n",
    "\n",
    "# En este modelo simple nuestra red tendra un input de imagen, 6 output de clase y convoluciones de 3x3\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Definimos la red\n",
    "        # kernel (Nucleo de la red)\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)  #definimos la primera layer convolucional\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # Realizamos una operacion lineal con redes de neuronas lineales (y = wx + b)\n",
    "        \n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 por la dimension de las imagenes\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    #definimos lo que ocurre a las salidas de las capas\n",
    "    def forward(self, x):\n",
    "        # Max pooling sobre una ventana 2 x 2\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # Si el tamaño es cuadrado solo se puede definir una ventana\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        \n",
    "        # definimos la rectificacion sobre las capas lineales\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        #generamos la salida\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    # calculamos el total de features\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # Todas las dimensiones excepto el lote de entrada\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "# imprimimos nuestra red\n",
    "net = Net()\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que reducimos considerablemente el numero de features a calcular en la ultima layer. la gracia de pytorch es que una vez definida la funcion de avance, se calcula se forma automatica la funcion de backward para realizar la backpropagation de los pesos. \n",
    "\n",
    "Ahora analizamos cuales son los parametros de aprendizaje que soporta nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # Pesos de las convoluciones*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora probemos darle de input una \"\" imagen \"\" de 32x32 de 2 dimensiones generada aleatoriamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0133, -0.0326,  0.0448, -0.0999,  0.0685,  0.0557,  0.1693, -0.1791,\n",
      "         -0.0557, -0.0451]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora realizamos la operacion de zero_grad, para realizar la backpropagation (vease Contenidos-Backpropagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nota\n",
    "Si bien hemos dicho que pytorch es maravilloso, tambien tiene algunas peculiaridades, una de las mas importantes a considerar es que torch.nn solo soporta mini lotes. esto significa que toda la libreria torch.nn solo soporta inputs que son lotes de inputs (samples) y nunca un solo elemento. Si quieres solo una dimension por ejemplo, has de nescesariamente agregar dimensionalidad falsa para que sea ejecutable\n",
    "\n",
    "\n",
    "### Explicaciones y Recapitulacion hasta el momento\n",
    "\n",
    "* torch.Tensor - array multidimensional con soporte para operaciones como backward(). ademas contiene el gradiente de la funcion de perdida\n",
    "\n",
    "* nn.Module - Modulo de red neuronal. Forma facil de guardar parametros\n",
    "\n",
    "* nn.Parameter - una especie de tensor, se asigna como parametro al asignarse un atributo al modulo\n",
    "\n",
    "* autograd.Function - genera forward() y backward() de la operacion de calculo de gradiente. toda operacion de tensores crea al menos un nodo se funciones simples que conecta el tensor creado y comprime su historial\n",
    "\n",
    "#### Hasta el momento hemos aprendido\n",
    "\n",
    "* Definir una red neuronal\n",
    "* Procesar input y realizar la backpropagation\n",
    "\n",
    "#### ¿Que haremos ahora?\n",
    "\n",
    "* Calcular la loss function\n",
    "* Actualizar los pesos de la red a travez de nuestros metodos anteriores\n",
    "\n",
    "\n",
    "### Computar la funcion de perdida\n",
    "\n",
    "Utilizaremos la funcion MSELoss o en español, error cuadratico medio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perdida de la funcion 0.9286238551139832\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ponemos un nombre a la red\n",
    "output = net(input)\n",
    "target = torch.randn(10)  # Definimos un target arbitrario\n",
    "target = target.view(1, -1)  # lo hacemos de misma dimension que el input\n",
    "\n",
    "# definimos el criterio de la funcion loss\n",
    "# es importante notar que MSE es built-in en pytorch, pero es perfectamente viable\n",
    "# utilizar funciones definidas por uno mismo.\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Calculamos el loss inicial de la red\n",
    "loss = criterion(output, target)\n",
    "print(f'Perdida de la funcion {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizamos la Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([-0.0108,  0.0120, -0.0192, -0.0005,  0.0094, -0.0021])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # hacemos el gradiente igual a zero\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos queda solo una cosa que hacer, Ajustar los pesos.\n",
    "Para esto haremos la operacion mas tradicional y simple de ajuste de parametros\n",
    "\n",
    "$$new\\_weight = weight - ratio\\_de\\_aprendizaje \\cdot gradiente$$\n",
    "\n",
    "ó, de forma formal\n",
    "\n",
    "$$w_{k_i+1} = w_{k_i} - h_{net} \\cdot \\nabla F$$\n",
    "\n",
    "con $\\nabla F = \\frac {\\sum_i^N \\nabla f_i}{N} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante ir analizando constantemente los resultados, para observar si existe posibilidad de optimizacion con los diversos metodos existentes. Por ahora utilizaremos el metodo SGD, o de optimizacion por disminucion de gradiente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Y ya esta, creaste tu primera red neuronal convolucional. Ahora continuaremos con aplicar una red similar ha un modelo predictivo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
